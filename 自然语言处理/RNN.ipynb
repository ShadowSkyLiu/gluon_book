{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/_lib/decorator.py:205: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  first = inspect.getargspec(caller)[0][0]  # first arg\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import loss as gloss\n",
    "import random\n",
    "import zipfile\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每天在想想想想著你\\n这'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with zipfile.ZipFile('/home/universe/.mxnet/datasets/data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('/home/universe/.mxnet/datasets/data/')\n",
    "\n",
    "with open('/home/universe/.mxnet/datasets/data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read()\n",
    "\n",
    "corpus_chars[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63282"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每天在想想想想著你 这'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:20000]\n",
    "corpus_chars[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1447"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: \n",
      " 想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n",
      "\n",
      "indices: \n",
      " [886, 1045, 746, 103, 109, 1214, 29, 886, 1045, 1176, 431, 1297, 315, 422, 1, 1220, 29, 886, 1045, 1176, 431, 907, 1033, 1203, 558, 953, 29, 907, 1033, 1203, 422, 1, 704, 29, 786, 1254, 522, 1254, 522, 1254]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:40]\n",
    "print('chars: \\n', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('\\nindices: \\n', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    num_inputs = len(corpus_indices) - 1\n",
    "    num_examples = num_inputs // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "#     print(num_examples, epoch_size)\n",
    "    \n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "#     print(example_indices)\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos*num_steps: (pos+1)*num_steps]\n",
    "    for i in range(epoch_size):\n",
    "        i = i * batch_size\n",
    "#         print('i=',i)\n",
    "        batch_indices = example_indices[i: i+batch_size]\n",
    "        X = nd.array([_data(j) for j in batch_indices], ctx=ctx)\n",
    "        Y = nd.array([_data(j+1) for j in batch_indices], ctx=ctx)\n",
    "        yield X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0: batch_size*batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    # 减一是因为输出的索引是相应输入的索引加一。\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i+num_steps]\n",
    "        Y = indices[:, i+1: i+num_steps+1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seq = list(range(30))\n",
    "# for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=3):\n",
    "#     print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[15. 12.]\n",
      " [16. 13.]\n",
      " [17. 14.]]\n",
      "<NDArray 3x2 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, (2, 1447))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_onehot(X, size):\n",
    "    return [nd.one_hot(x, size) for x in X.T]\n",
    "\n",
    "print(X.T)\n",
    "get_inputs = to_onehot\n",
    "inputs = get_inputs(X, vocab_size)\n",
    "len(inputs), inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "ctx = utils.try_gpu()\n",
    "print('will use', ctx)\n",
    "\n",
    "num_inputs = vocab_size\n",
    "num_hiddens = 256\n",
    "num_outputs = vocab_size\n",
    "\n",
    "def get_params():\n",
    "    # 隐藏层参数。\n",
    "    W_xh = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens),\n",
    "                            ctx=ctx)\n",
    "    W_hh = nd.random.normal(scale=0.01, shape=(num_hiddens, num_hiddens),\n",
    "                            ctx=ctx)\n",
    "    b_h = nd.zeros(num_hiddens, ctx=ctx)\n",
    "    # 输出层参数。\n",
    "    W_hy = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs),\n",
    "                            ctx=ctx)\n",
    "    b_y = nd.zeros(num_outputs, ctx=ctx)\n",
    "\n",
    "    params = [W_xh, W_hh, b_h, W_hy, b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, *params):\n",
    "    H = state\n",
    "    W_xh, W_hh, b_h, W_hy, b_y = params\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return outputs, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, (2, 1447), (2, 256))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = nd.zeros(shape=(X.shape[0], num_hiddens), ctx=ctx)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(get_inputs(X.as_in_context(ctx), vocab_size), state,\n",
    "                         *params)\n",
    "len(outputs), outputs[0].shape, state_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下函数预测基于前缀prefix接下来的num_chars个字符\n",
    "def predict_rnn(rnn, prefix, num_chars, params, num_hiddens, vocab_size, ctx,\n",
    "               idx_to_char, char_to_idx, get_inputs, is_lstm=False):\n",
    "    prefix = prefix.lower()\n",
    "    state_h = nd.zeros(shape=(1, num_hiddens), ctx=ctx)\n",
    "    if is_lstm:\n",
    "        state_c = nd.zeros(shape=(1, num_hiddens), ctx=ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars + len(prefix)):\n",
    "        X = nd.array([output[-1]], ctx=ctx)\n",
    "        if is_lstm:\n",
    "            Y, state_h, state_c = rnn(getinputs(X, vocab_size), state_h, state_c, *params)\n",
    "        else:\n",
    "            Y, state_h = rnn(get_inputs(X, vocab_size), state_h, *params)\n",
    "        if i < len(prefix) - 1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else: \n",
    "            next_input = int(Y[0].argmax(axis=1).asscalar())\n",
    "        output.append(next_input)\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_rnn(rnn, prefix, num_chars, params, num_hiddens, vocab_size, ctx,\n",
    "#                 idx_to_char, char_to_idx, get_inputs, is_lstm=False):\n",
    "#     prefix = prefix.lower()\n",
    "#     state_h = nd.zeros(shape=(1, num_hiddens), ctx=ctx)\n",
    "#     if is_lstm:\n",
    "#         # 当 RNN 使用 LSTM 时才会用到（后面章节会介绍），本节可以忽略。\n",
    "#         state_c = nd.zeros(shape=(1, num_hiddens), ctx=ctx)\n",
    "#     output = [char_to_idx[prefix[0]]]\n",
    "#     for i in range(num_chars + len(prefix)):\n",
    "#         X = nd.array([output[-1]], ctx=ctx)\n",
    "#         # 在序列中循环迭代隐藏状态。\n",
    "#         if is_lstm:\n",
    "#             # 当 RNN 使用 LSTM 时才会用到（后面章节会介绍），本节可以忽略。\n",
    "#             Y, state_h, state_c = rnn(get_inputs(X, vocab_size), state_h,\n",
    "#                                       state_c, *params)\n",
    "#         else:\n",
    "#             Y, state_h = rnn(get_inputs(X, vocab_size), state_h, *params)\n",
    "#         if i < len(prefix) - 1:\n",
    "#             next_input = char_to_idx[prefix[i + 1]]\n",
    "#         else:\n",
    "#             next_input = int(Y[0].argmax(axis=1).asscalar())\n",
    "#         output.append(next_input)\n",
    "#     return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, state_h, Y, theta, ctx):\n",
    "    if theta is not None:\n",
    "        norm = nd.array([0.0], ctx)\n",
    "        for param in params:\n",
    "            norm += (param.grad ** 2).sum()\n",
    "        norm = norm.sqrt().asscalar()\n",
    "        if norm > theta:\n",
    "            for param in params:\n",
    "                param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(rnn, is_random_iter, num_epochs, num_steps,\n",
    "                          num_hiddens, lr, clipping_theta, batch_size,\n",
    "                          vocab_size, pred_period, pred_len, prefixes,\n",
    "                          get_params, get_inputs, ctx, corpus_indices,\n",
    "                          idx_to_char, char_to_idx, is_lstm=False):\n",
    "    if is_random_iter:\n",
    "        data_iter = data_iter_random\n",
    "    else: \n",
    "        data_iter = data_iter_consecutive\n",
    "    \n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        if not is_random_iter:\n",
    "            state_h = nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx)\n",
    "            if is_lstm:\n",
    "                state_c = nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx)\n",
    "        train_l_sum = nd.array([0], ctx=ctx)\n",
    "        train_l_cnt = 0\n",
    "        for X, Y in data_iter(corpus_indices, batch_size, num_steps, ctx):\n",
    "            if is_random_iter:\n",
    "                state_h = nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx)\n",
    "                if is_lstm:\n",
    "                    state_c = nd.zeros(shape=(batch_size, num_hiddens),\n",
    "                                       ctx=ctx)\n",
    "            else:\n",
    "                state_h = state_h.detach()\n",
    "                if is_lstm:\n",
    "                    state_c = state_c.detach()\n",
    "            with autograd.record():\n",
    "                if is_lstm:\n",
    "                    outputs, state_h, state_c = rnn(\n",
    "                        get_inputs(X, vocab_size), state_h, state_c, *params)\n",
    "                else: \n",
    "                    outputs, state_h = rnn(\n",
    "                        get_inputs(X, vocab_size), state_h, *params)\n",
    "                # 设 t_ib_j 为时间步 i 批量中的元素 j：\n",
    "                # y 形状：（batch_size * num_steps,）\n",
    "                # y = [t_0b_0, t_0b_1, ..., t_1b_0, t_1b_1, ..., ]。\n",
    "                y = Y.T.reshape((-1,))\n",
    "#                 print (y)\n",
    "                # 拼接 outputs，形状：(batch_size * num_steps, vocab_size)。\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                l = loss(outputs, y)\n",
    "            l.backward()\n",
    "             # 裁剪梯度。\n",
    "            grad_clipping(params, state_h, Y, clipping_theta, ctx)\n",
    "            utils.SGD(params, lr)\n",
    "            train_l_sum = train_l_sum + l.sum()\n",
    "            train_l_cnt += l.size\n",
    "        if epoch % pred_period == 0:\n",
    "            print(\"\\nepoch %d, perplexity %f\"\n",
    "                  % (epoch, (train_l_sum / train_l_cnt).exp().asscalar()))\n",
    "            for prefix in prefixes:\n",
    "                print(' - ', predict_rnn(\n",
    "                    rnn, prefix, pred_len, params, num_hiddens, vocab_size,\n",
    "                    ctx, idx_to_char, char_to_idx, get_inputs, is_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 40, perplexity 65.488914\n",
      " -  分开 我不要再样 我不要再不 我不能再不 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再\n",
      " -  不分开 一直用剧 我想了这样  哼有我们留 你一种味道 有一种味道 他不是我们 你不能 不想我 别你的公式我 不知我 别怪我 别你的公式我 不知我 别怪我 别你的公式我 不知我 别怪我 别你的公式我 不知我 \n",
      "\n",
      "epoch 80, perplexity 9.116345\n",
      " -  分开 我想要陪你走着 一身好酒 你面一碗热粥 配上几斤的牛肉 我说店小二 三两银够不够 景色入秋 漫天黄沙截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍\n",
      " -  不分开 我的想 放你载睛看着我 别发抖 快给我抬起头 有话去对医药箱一场悲 我不能 想不不到陪我 不散为真 我该好这生活 后知后觉 又使了双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使\n",
      "\n",
      "epoch 120, perplexity 3.701919\n",
      " -  分开了到 学许这种和著 篮使将那的画 还过着你还得这样 我想就这样  你说不起不是 轻静了我的微知 没说能在去的我 该要这力再的背 我想要你拆远开著我 爱不休 别给我抬起头 有话去对医药箱说 别怪我 别怪我\n",
      " -  不分开 我没有这你 我的爱空 被话着外的溪边我 我们不见就整去前 不想再笑 你耀的背我感多的可爱女人 透坏的让我疯狂的可爱女人 透坏的让我疯狂的可爱女人 透坏的让我疯狂的可爱女人 透坏的让我疯狂的可爱女人 透\n",
      "\n",
      "epoch 160, perplexity 2.601635\n",
      " -  分开 我想要再你 我不着我不要 又情的欢火我 你的灵应 你想靠很 乐天风通 过人 在箩  有炭么 太快的手如就 包我的天一来 还室的那一间 我怎么看不见 消失的下雨天 我好想再淋一遍 没想到失去的勇气我还留\n",
      " -  不分开  没有你有我说你要没有难样 拜托得大事我们不起很年幼 而如 我想是你的脑袋有问题上 但不到的茶 在一种味道叫做家 他法泡剔它 喝感时觉还不差 陆羽泡的茶 听说名和利都不拿 他牵着一匹瘦马在走天涯 爷爷\n",
      "\n",
      "epoch 200, perplexity 2.269988\n",
      " -  分开 我想要陪你 我不要说想 我太帮 爱情 是手前久了吧? 折一九好柳诚你 难不会我都腔开 是因一种我给忧的爱  被你情依自 不想多这样 不象 是你的是 还小是干 说杰河 旧静她人 快打那中的溪边 情默纯动\n",
      " -  不分开 你没有这样边 爸天不要 我已了这节奏 后知后觉 我该不好生活 我知好好生活 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 我永在赢分道 \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "num_steps = 35\n",
    "batch_size = 32\n",
    "lr = 0.2\n",
    "clipping_theta = 5\n",
    "prefixes = ['分开', '不分开']\n",
    "pred_period = 40\n",
    "pred_len = 100\n",
    "train_and_predict_rnn(rnn, False, num_epochs, num_steps, num_hiddens, lr,\n",
    "                      clipping_theta, batch_size, vocab_size, pred_period,\n",
    "                      pred_len, prefixes, get_params, get_inputs, ctx,\n",
    "                      corpus_indices, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(rnn, is_random_iter, num_epochs, num_steps,\n",
    "                          num_hiddens, lr, clipping_theta, batch_size,\n",
    "                          vocab_size, pred_period, pred_len, prefixes,\n",
    "                          get_params, get_inputs, ctx, corpus_indices,\n",
    "                          idx_to_char, char_to_idx, is_lstm=False):\n",
    "    if is_random_iter:\n",
    "        data_iter = data_iter_random\n",
    "    else:\n",
    "        data_iter = data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # 如使用相邻采样，隐藏变量只需在该 epoch 开始时初始化。\n",
    "        if not is_random_iter:\n",
    "            state_h = nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx)\n",
    "            if is_lstm:\n",
    "                state_c = nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx)\n",
    "        train_l_sum = nd.array([0], ctx=ctx)\n",
    "        train_l_cnt = 0\n",
    "        for X, Y in data_iter(corpus_indices, batch_size, num_steps, ctx):\n",
    "            # 如使用随机采样，读取每个随机小批量前都需要初始化隐藏变量。\n",
    "            if is_random_iter:\n",
    "                state_h = nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx)\n",
    "                if is_lstm:\n",
    "                    state_c = nd.zeros(shape=(batch_size, num_hiddens),\n",
    "                                       ctx=ctx)\n",
    "            # 如使用相邻采样，需要使用 detach 函数从计算图分离隐藏状态变量。\n",
    "            else:\n",
    "                state_h = state_h.detach()\n",
    "                if is_lstm:\n",
    "                    state_c = state_c.detach()\n",
    "            with autograd.record():\n",
    "                # outputs 形状：(batch_size, vocab_size)。\n",
    "                if is_lstm:\n",
    "                    outputs, state_h, state_c = rnn(\n",
    "                        get_inputs(X, vocab_size), state_h, state_c, *params)\n",
    "                else:\n",
    "                    outputs, state_h = rnn(\n",
    "                        get_inputs(X, vocab_size), state_h, *params)\n",
    "                # 设 t_ib_j 为时间步 i 批量中的元素 j：\n",
    "                # y 形状：（batch_size * num_steps,）\n",
    "                # y = [t_0b_0, t_0b_1, ..., t_1b_0, t_1b_1, ..., ]。\n",
    "                y = Y.T.reshape((-1,))\n",
    "                # 拼接 outputs，形状：(batch_size * num_steps, vocab_size)。\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                l = loss(outputs, y)\n",
    "            l.backward()\n",
    "            # 裁剪梯度。\n",
    "            grad_clipping(params, state_h, Y, clipping_theta, ctx)\n",
    "            gb.sgd(params, lr, 1)\n",
    "            train_l_sum = train_l_sum + l.sum()\n",
    "            train_l_cnt += l.size\n",
    "        if epoch % pred_period == 0:\n",
    "            print(\"\\nepoch %d, perplexity %f\"\n",
    "                  % (epoch, (train_l_sum / train_l_cnt).exp().asscalar()))\n",
    "            for prefix in prefixes:\n",
    "                print(' - ', predict_rnn(\n",
    "                    rnn, prefix, pred_len, params, num_hiddens, vocab_size,\n",
    "                    ctx, idx_to_char, char_to_idx, get_inputs, is_lstm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
